{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Buying KNN Experiment\n",
    "\n",
    "Standalone KNN experiment extracted from `house_buying_nn_experiment.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Helps avoid MKL KMeans warnings on Windows\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.cluster._kmeans\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = Path.cwd()\n",
    "if not (PROJECT_DIR / \"csv\").exists():\n",
    "    PROJECT_DIR = PROJECT_DIR.parent\n",
    "\n",
    "DATA_DIR = PROJECT_DIR / \"csv\"\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR / \"house_buy_train.csv\")\n",
    "cv_df = pd.read_csv(DATA_DIR / \"house_buy_cv.csv\")\n",
    "test_df = pd.read_csv(DATA_DIR / \"house_buy_test.csv\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"buyer_income_lpa\",\n",
    "    \"house_price_lakh\",\n",
    "    \"loan_eligibility\",\n",
    "    \"credit_score\",\n",
    "    \"down_payment_percent\",\n",
    "    \"existing_emi_lpa\",\n",
    "    \"employment_years\",\n",
    "    \"dependents\",\n",
    "    \"property_location_score\",\n",
    "    \"employment_type\",\n",
    "]\n",
    "\n",
    "numeric_cols = [\n",
    "    \"buyer_income_lpa\",\n",
    "    \"house_price_lakh\",\n",
    "    \"credit_score\",\n",
    "    \"down_payment_percent\",\n",
    "    \"existing_emi_lpa\",\n",
    "    \"employment_years\",\n",
    "    \"dependents\",\n",
    "    \"property_location_score\",\n",
    "]\n",
    "\n",
    "cat_cols = [\"loan_eligibility\", \"employment_type\"]\n",
    "\n",
    "label_to_int = {\"no\": 0, \"neutral\": 1, \"yes\": 2}\n",
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "class_order = np.array([0, 1, 2])\n",
    "class_names = [int_to_label[i] for i in class_order]\n",
    "\n",
    "X_train_raw = train_df[feature_cols]\n",
    "y_train = train_df[\"can_buy\"].map(label_to_int).to_numpy()\n",
    "X_cv_raw = cv_df[feature_cols]\n",
    "y_cv = cv_df[\"can_buy\"].map(label_to_int).to_numpy()\n",
    "X_test_raw = test_df[feature_cols]\n",
    "y_test = test_df[\"can_buy\"].map(label_to_int).to_numpy()\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"CV shape:\", cv_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Class distribution (train):\", train_df[\"can_buy\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train_raw)\n",
    "X_cv = preprocessor.transform(X_cv_raw)\n",
    "X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "# Convert to dense arrays so every model (including custom clustering models) can consume the same inputs.\n",
    "X_train = X_train.toarray() if hasattr(X_train, \"toarray\") else np.asarray(X_train)\n",
    "X_cv = X_cv.toarray() if hasattr(X_cv, \"toarray\") else np.asarray(X_cv)\n",
    "X_test = X_test.toarray() if hasattr(X_test, \"toarray\") else np.asarray(X_test)\n",
    "\n",
    "print(\"Processed feature count:\", X_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansLabelModel:\n",
    "    def __init__(self, n_clusters=7, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y, n_classes=3):\n",
    "        self.model = KMeans(\n",
    "            n_clusters=self.n_clusters,\n",
    "            init=\"k-means++\",\n",
    "            n_init=20,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        cluster_ids = self.model.fit_predict(X)\n",
    "        self.classes_ = np.arange(n_classes)\n",
    "        self.cluster_proba_ = np.zeros((self.n_clusters, n_classes), dtype=float)\n",
    "\n",
    "        for c in range(self.n_clusters):\n",
    "            idx = np.where(cluster_ids == c)[0]\n",
    "            if len(idx) == 0:\n",
    "                self.cluster_proba_[c] = np.ones(n_classes) / n_classes\n",
    "            else:\n",
    "                counts = np.bincount(y[idx], minlength=n_classes).astype(float)\n",
    "                self.cluster_proba_[c] = (counts + 1.0) / (counts.sum() + n_classes)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        nearest_cluster = self.model.predict(X)\n",
    "        return self.cluster_proba_[nearest_cluster]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(axis=1)\n",
    "\n",
    "\n",
    "class KMedoidsLabelModel:\n",
    "    def __init__(self, n_clusters=7, random_state=42, max_iter=30):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, n_classes=3):\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        k = min(self.n_clusters, n_samples)\n",
    "\n",
    "        medoid_idx = rng.choice(n_samples, size=k, replace=False)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            d2 = ((X[:, None, :] - X[medoid_idx][None, :, :]) ** 2).sum(axis=2)\n",
    "            assignments = d2.argmin(axis=1)\n",
    "\n",
    "            changed = False\n",
    "            new_medoid_idx = medoid_idx.copy()\n",
    "\n",
    "            for c in range(k):\n",
    "                pts = np.where(assignments == c)[0]\n",
    "                if len(pts) == 0:\n",
    "                    continue\n",
    "                cluster_pts = X[pts]\n",
    "                within_cluster_d2 = ((cluster_pts[:, None, :] - cluster_pts[None, :, :]) ** 2).sum(axis=2)\n",
    "                best_local_pt = pts[within_cluster_d2.sum(axis=1).argmin()]\n",
    "                if best_local_pt != medoid_idx[c]:\n",
    "                    changed = True\n",
    "                    new_medoid_idx[c] = best_local_pt\n",
    "\n",
    "            medoid_idx = new_medoid_idx\n",
    "            if not changed:\n",
    "                break\n",
    "\n",
    "        self.medoids_ = X[medoid_idx]\n",
    "        self.classes_ = np.arange(n_classes)\n",
    "\n",
    "        d2_final = ((X[:, None, :] - self.medoids_[None, :, :]) ** 2).sum(axis=2)\n",
    "        assignments = d2_final.argmin(axis=1)\n",
    "\n",
    "        self.cluster_proba_ = np.zeros((k, n_classes), dtype=float)\n",
    "        for c in range(k):\n",
    "            idx = np.where(assignments == c)[0]\n",
    "            if len(idx) == 0:\n",
    "                self.cluster_proba_[c] = np.ones(n_classes) / n_classes\n",
    "            else:\n",
    "                counts = np.bincount(y[idx], minlength=n_classes).astype(float)\n",
    "                self.cluster_proba_[c] = (counts + 1.0) / (counts.sum() + n_classes)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        d2 = ((X[:, None, :] - self.medoids_[None, :, :]) ** 2).sum(axis=2)\n",
    "        nearest_medoid = d2.argmin(axis=1)\n",
    "        return self.cluster_proba_[nearest_medoid]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(axis=1)\n",
    "\n",
    "\n",
    "class SeedAveragedMLP:\n",
    "    def __init__(self, base_params, seeds):\n",
    "        self.base_params = dict(base_params)\n",
    "        self.seeds = list(seeds)\n",
    "        self.models = []\n",
    "        self.classes_ = np.array([0, 1, 2])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        for seed in self.seeds:\n",
    "            model = MLPClassifier(random_state=seed, **self.base_params)\n",
    "            model.fit(X, y)\n",
    "            self.models.append(model)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self.models:\n",
    "            raise RuntimeError(\"SeedAveragedMLP must be fitted before prediction.\")\n",
    "\n",
    "        n_classes = len(self.classes_)\n",
    "        avg = np.zeros((X.shape[0], n_classes), dtype=float)\n",
    "        for model in self.models:\n",
    "            proba = model.predict_proba(X)\n",
    "            aligned = np.zeros((X.shape[0], n_classes), dtype=float)\n",
    "            for col_idx, cls in enumerate(model.classes_):\n",
    "                aligned[:, int(cls)] = proba[:, col_idx]\n",
    "            avg += aligned\n",
    "        return avg / len(self.models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac740ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_models(seed):\n",
    "    return {\n",
    "        \"nn_small\": SeedAveragedMLP(\n",
    "            base_params={\n",
    "                \"hidden_layer_sizes\": (24,),\n",
    "                \"activation\": \"relu\",\n",
    "                \"solver\": \"adam\",\n",
    "                \"alpha\": 0.01,\n",
    "                \"learning_rate_init\": 0.0012,\n",
    "                \"max_iter\": 2200,\n",
    "                \"early_stopping\": True,\n",
    "                \"validation_fraction\": 0.2,\n",
    "                \"n_iter_no_change\": 25,\n",
    "            },\n",
    "            seeds=[seed + 1, seed + 11, seed + 21, seed + 31, seed + 41],\n",
    "        ),\n",
    "        \"nn_medium\": MLPClassifier(\n",
    "            hidden_layer_sizes=(44, 22),\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            alpha=0.02,\n",
    "            learning_rate_init=0.0007,\n",
    "            max_iter=2600,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.22,\n",
    "            n_iter_no_change=20,\n",
    "            random_state=seed + 3,\n",
    "        ),\n",
    "        \"nn_deep\": MLPClassifier(\n",
    "            hidden_layer_sizes=(56, 28, 14),\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            alpha=0.012,\n",
    "            learning_rate_init=0.001,\n",
    "            max_iter=2800,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.2,\n",
    "            n_iter_no_change=25,\n",
    "            random_state=seed + 4,\n",
    "        ),\n",
    "        \"knn\": KNeighborsClassifier(n_neighbors=11, weights=\"distance\"),\n",
    "        \"kmeanspp\": KMeansLabelModel(n_clusters=9, random_state=seed + 5),\n",
    "        \"kmedoids\": KMedoidsLabelModel(n_clusters=9, random_state=seed + 6, max_iter=35),\n",
    "    }\n",
    "\n",
    "\n",
    "def fit_model(model, X, y, n_classes):\n",
    "    if isinstance(model, (KMeansLabelModel, KMedoidsLabelModel)):\n",
    "        model.fit(X, y, n_classes=n_classes)\n",
    "    else:\n",
    "        model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_proba_full(model, X, n_classes):\n",
    "    proba = model.predict_proba(X)\n",
    "\n",
    "    if proba.shape[1] == n_classes:\n",
    "        return proba\n",
    "\n",
    "    full = np.zeros((proba.shape[0], n_classes), dtype=float)\n",
    "    if hasattr(model, \"classes_\"):\n",
    "        for col_idx, cls in enumerate(model.classes_):\n",
    "            full[:, int(cls)] = proba[:, col_idx]\n",
    "    else:\n",
    "        full[:, : proba.shape[1]] = proba\n",
    "    row_sum = full.sum(axis=1, keepdims=True)\n",
    "    row_sum[row_sum == 0] = 1.0\n",
    "    return full / row_sum\n",
    "\n",
    "\n",
    "def evaluate_split(y_true, y_pred, y_proba, n_classes):\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision_macro\": precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)[0],\n",
    "        \"recall_macro\": precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)[1],\n",
    "        \"f1_macro\": precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)[2],\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    labels = np.arange(n_classes)\n",
    "    y_true_bin = label_binarize(y_true, classes=labels)\n",
    "\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = log_loss(y_true, y_proba, labels=labels)\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = np.nan\n",
    "\n",
    "    try:\n",
    "        metrics[\"roc_auc_macro_ovr\"] = roc_auc_score(y_true_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_macro_ovr\"] = np.nan\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model_on_splits(model, splits, n_classes):\n",
    "    pred_store = {}\n",
    "    proba_store = {}\n",
    "    rows = []\n",
    "\n",
    "    for split_name, (X_split, y_split) in splits.items():\n",
    "        split_proba = predict_proba_full(model, X_split, n_classes=n_classes)\n",
    "        split_pred = split_proba.argmax(axis=1)\n",
    "\n",
    "        pred_store[split_name] = split_pred\n",
    "        proba_store[split_name] = split_proba\n",
    "\n",
    "        row = evaluate_split(y_split, split_pred, split_proba, n_classes=n_classes)\n",
    "        row[\"split\"] = split_name\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows, pred_store, proba_store\n",
    "\n",
    "\n",
    "def select_ensemble_families(base_metrics_df, top_k=2):\n",
    "    cv_rank = (\n",
    "        base_metrics_df[base_metrics_df[\"split\"] == \"cv\"]\n",
    "        .sort_values(\"f1_macro\", ascending=False)\n",
    "    )\n",
    "    return cv_rank.head(top_k)[\"model\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    \"train\": (X_train, y_train),\n",
    "    \"cv\": (X_cv, y_cv),\n",
    "    \"test\": (X_test, y_test),\n",
    "}\n",
    "\n",
    "n_classes = len(class_order)\n",
    "model = KNeighborsClassifier(n_neighbors=11, weights=\"distance\")\n",
    "fit_model(model, X_train, y_train, n_classes=n_classes)\n",
    "\n",
    "rows, pred_store, _ = evaluate_model_on_splits(model, splits, n_classes=n_classes)\n",
    "knn_metrics_df = pd.DataFrame(rows)\n",
    "\n",
    "metric_cols = [\n",
    "    \"accuracy\",\n",
    "    \"precision_macro\",\n",
    "    \"recall_macro\",\n",
    "    \"f1_macro\",\n",
    "    \"balanced_accuracy\",\n",
    "    \"log_loss\",\n",
    "    \"roc_auc_macro_ovr\",\n",
    "]\n",
    "\n",
    "display(knn_metrics_df.set_index(\"split\")[metric_cols])\n",
    "print(\"\\nTest classification report (KNN):\")\n",
    "print(classification_report(y_test, pred_store[\"test\"], target_names=class_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
